---
title: "ML Pipeline Overview"
sidebar_position: 1
description: "Comprehensive documentation of the machine learning pipeline for anomaly detection and analysis"
---

# Machine Learning Pipeline

## Abstract

The CidadÃ£o.AI ML Pipeline implements a sophisticated **MLOps-ready framework** for training, validating, and deploying machine learning models specialized in government data analysis. The pipeline integrates **ensemble methods**, **deep learning architectures**, and **explainable AI** techniques to achieve state-of-the-art performance in anomaly detection and pattern recognition.

## Architecture Overview

```mermaid
graph TB
    subgraph "Data Ingestion"
        DI[Data Collectors]
        DP[Data Preprocessors]
        DV[Data Validators]
    end
    
    subgraph "Feature Engineering"
        FE[Feature Extractors]
        FS[Feature Selectors]
        FT[Feature Transformers]
    end
    
    subgraph "Model Training"
        MT[Model Trainers]
        MV[Model Validators]
        HP[Hyperparameter Tuning]
    end
    
    subgraph "Model Deployment"
        MS[Model Servers]
        MC[Model Cache]
        MM[Model Monitoring]
    end
    
    subgraph "MLOps Infrastructure"
        ML[MLflow Tracking]
        WB[Weights & Biases]
        MR[Model Registry]
    end
    
    DI --> DP
    DP --> DV
    DV --> FE
    FE --> FS
    FS --> FT
    FT --> MT
    MT --> MV
    MV --> HP
    HP --> MS
    MS --> MC
    MS --> MM
    
    MT --> ML
    MV --> WB
    MS --> MR
```

## Core Components

### Data Pipeline

#### Data Collection

```python
class DataCollector:
    """Collects and normalizes data from multiple government sources."""
    
    def __init__(self):
        self.sources = {
            'portal_transparencia': PortalTransparenciaAPI(),
            'tcu': TCUDataAPI(),
            'cgu': CGUDataAPI(),
            'compras_gov': ComprasGovAPI()
        }
    
    async def collect_batch(
        self, 
        source: str, 
        date_range: DateRange,
        filters: Dict[str, Any]
    ) -> pd.DataFrame:
        """Collect batch data with validation and normalization."""
        
        raw_data = await self.sources[source].fetch(date_range, filters)
        
        # Validate data quality
        quality_score = self.validate_quality(raw_data)
        if quality_score < 0.8:
            raise DataQualityError(f"Quality score {quality_score} below threshold")
        
        # Normalize schema
        normalized_data = self.normalize_schema(raw_data, source)
        
        return normalized_data
```

#### Feature Engineering

```python
class AdvancedFeatureEngineer:
    """Sophisticated feature engineering for government data."""
    
    def __init__(self):
        self.transformers = {
            'numerical': NumericalTransformer(),
            'categorical': CategoricalTransformer(),
            'temporal': TemporalTransformer(),
            'textual': TextualTransformer(),
            'graph': GraphTransformer()
        }
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create comprehensive feature set."""
        
        features = pd.DataFrame(index=df.index)
        
        # Statistical features
        features = self._add_statistical_features(features, df)
        
        # Temporal patterns
        features = self._add_temporal_features(features, df)
        
        # Network features
        features = self._add_network_features(features, df)
        
        # NLP features
        features = self._add_nlp_features(features, df)
        
        return features
    
    def _add_statistical_features(self, features: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:
        """Add statistical anomaly indicators."""
        
        # Z-score based features
        for col in df.select_dtypes(include=[np.number]).columns:
            features[f'{col}_zscore'] = np.abs(stats.zscore(df[col]))
            features[f'{col}_iqr_outlier'] = self._detect_iqr_outliers(df[col])
        
        # Mahalanobis distance
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        if len(numerical_cols) > 1:
            features['mahalanobis_distance'] = self._compute_mahalanobis(df[numerical_cols])
        
        return features
```

### Model Architecture

#### Ensemble Framework

```python
class AnomalyEnsemble(nn.Module):
    """Ensemble model combining multiple detection algorithms."""
    
    def __init__(self, input_dim: int, hidden_dims: List[int]):
        super().__init__()
        
        # Individual models
        self.isolation_forest = IsolationForestWrapper()
        self.one_class_svm = OneClassSVMWrapper()
        self.autoencoder = AutoencoderAnomalyDetector(input_dim, hidden_dims)
        self.lstm_detector = LSTMAnomalyDetector(input_dim)
        
        # Meta-learner
        self.meta_learner = nn.Sequential(
            nn.Linear(4, 16),  # 4 base models
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through ensemble."""
        
        # Get predictions from base models
        if_score = self.isolation_forest(x)
        svm_score = self.one_class_svm(x)
        ae_score = self.autoencoder(x)
        lstm_score = self.lstm_detector(x)
        
        # Stack base predictions
        base_predictions = torch.stack([if_score, svm_score, ae_score, lstm_score], dim=1)
        
        # Meta-learner combines predictions
        ensemble_score = self.meta_learner(base_predictions)
        
        return ensemble_score
```

#### Autoencoder Architecture

```python
class AutoencoderAnomalyDetector(nn.Module):
    """Variational Autoencoder for anomaly detection."""
    
    def __init__(self, input_dim: int, hidden_dims: List[int], latent_dim: int = 32):
        super().__init__()
        
        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim
        
        self.encoder = nn.Sequential(*encoder_layers)
        
        # Latent space
        self.mu_layer = nn.Linear(prev_dim, latent_dim)
        self.logvar_layer = nn.Linear(prev_dim, latent_dim)
        
        # Decoder
        decoder_layers = []
        prev_dim = latent_dim
        for hidden_dim in reversed(hidden_dims):
            decoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim
        
        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        self.decoder = nn.Sequential(*decoder_layers)
    
    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Encode input to latent space."""
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar
    
    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """Reparameterization trick."""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Forward pass through VAE."""
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        reconstruction = self.decoder(z)
        
        # Anomaly score based on reconstruction error
        reconstruction_error = F.mse_loss(reconstruction, x, reduction='none').mean(dim=1)
        
        return reconstruction, mu, logvar, reconstruction_error
```

### Training Pipeline

#### Advanced Training Loop

```python
class AdvancedMLPipeline:
    """Production-ready ML pipeline with MLOps integration."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = structlog.get_logger(__name__)
        
        # MLOps integrations
        if MLFLOW_AVAILABLE:
            mlflow.set_tracking_uri(config.get('mlflow_uri', 'sqlite:///mlruns.db'))
        
        if WANDB_AVAILABLE and config.get('use_wandb', False):
            wandb.init(project='cidadao-ai-ml', config=config)
    
    async def train_model(
        self, 
        model_type: ModelType,
        train_data: pd.DataFrame,
        val_data: pd.DataFrame,
        config: Dict[str, Any]
    ) -> TrainingRun:
        """Train model with comprehensive tracking."""
        
        run_id = self._generate_run_id()
        training_run = TrainingRun(
            id=run_id,
            model_type=model_type,
            status=TrainingStatus.PREPROCESSING,
            config=config
        )
        
        try:
            # Start MLflow run
            with mlflow.start_run(run_name=f"{model_type.value}_{run_id}"):
                
                # Log parameters
                mlflow.log_params(config)
                
                # Preprocess data
                X_train, y_train = self._preprocess_data(train_data)
                X_val, y_val = self._preprocess_data(val_data)
                
                # Create model
                model = self._create_model(model_type, X_train.shape[1], config)
                
                # Training setup
                optimizer = optim.AdamW(
                    model.parameters(),
                    lr=config.get('learning_rate', 1e-3),
                    weight_decay=config.get('weight_decay', 1e-4)
                )
                
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, 
                    patience=config.get('patience', 10),
                    factor=0.5
                )
                
                # Training loop
                training_run.status = TrainingStatus.TRAINING
                best_val_loss = float('inf')
                
                for epoch in range(config.get('epochs', 100)):
                    # Training step
                    train_loss, train_metrics = await self._train_epoch(
                        model, X_train, y_train, optimizer
                    )
                    
                    # Validation step
                    val_loss, val_metrics = await self._validate_epoch(
                        model, X_val, y_val
                    )
                    
                    # Logging
                    self._log_epoch_metrics(epoch, train_loss, val_loss, train_metrics, val_metrics)
                    
                    # Learning rate scheduling
                    scheduler.step(val_loss)
                    
                    # Early stopping
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        self._save_checkpoint(model, optimizer, epoch, run_id)
                    
                    # Wandb logging
                    if WANDB_AVAILABLE:
                        wandb.log({
                            'epoch': epoch,
                            'train_loss': train_loss,
                            'val_loss': val_loss,
                            **train_metrics,
                            **{f'val_{k}': v for k, v in val_metrics.items()}
                        })
                
                # Final evaluation
                training_run.status = TrainingStatus.VALIDATING
                final_metrics = await self._final_evaluation(model, X_val, y_val)
                training_run.metrics = final_metrics
                training_run.status = TrainingStatus.COMPLETED
                training_run.completed_at = datetime.utcnow()
                
                # Log model
                mlflow.pytorch.log_model(model, "model")
                mlflow.log_metrics(final_metrics.__dict__)
                
                return training_run
                
        except Exception as e:
            training_run.status = TrainingStatus.FAILED
            training_run.error_message = str(e)
            self.logger.error("Training failed", error=str(e), run_id=run_id)
            raise
```

### Model Evaluation

#### Comprehensive Metrics

```python
class ModelEvaluator:
    """Comprehensive model evaluation with statistical tests."""
    
    def __init__(self):
        self.metrics_calculators = {
            'classification': ClassificationMetrics(),
            'anomaly_detection': AnomalyDetectionMetrics(),
            'regression': RegressionMetrics()
        }
    
    def evaluate_model(
        self, 
        model: nn.Module, 
        test_data: pd.DataFrame,
        task_type: str
    ) -> Dict[str, float]:
        """Evaluate model with comprehensive metrics."""
        
        # Get predictions
        predictions = self._get_predictions(model, test_data)
        true_labels = test_data['label'].values
        
        # Calculate metrics
        metrics = {}
        
        if task_type == 'anomaly_detection':
            metrics.update(self._calculate_anomaly_metrics(predictions, true_labels))
            metrics.update(self._calculate_threshold_metrics(predictions, true_labels))
            metrics.update(self._calculate_statistical_tests(predictions, true_labels))
        
        # Add interpretability metrics
        metrics.update(self._calculate_interpretability_metrics(model, test_data))
        
        return metrics
    
    def _calculate_anomaly_metrics(
        self, 
        predictions: np.ndarray, 
        true_labels: np.ndarray
    ) -> Dict[str, float]:
        """Calculate anomaly detection specific metrics."""
        
        # Find optimal threshold
        fpr, tpr, thresholds = roc_curve(true_labels, predictions)
        optimal_idx = np.argmax(tpr - fpr)
        optimal_threshold = thresholds[optimal_idx]
        
        # Binary predictions
        binary_preds = (predictions >= optimal_threshold).astype(int)
        
        return {
            'auc_roc': roc_auc_score(true_labels, predictions),
            'auc_pr': average_precision_score(true_labels, predictions),
            'precision': precision_score(true_labels, binary_preds),
            'recall': recall_score(true_labels, binary_preds),
            'f1_score': f1_score(true_labels, binary_preds),
            'optimal_threshold': optimal_threshold,
            'true_positive_rate': tpr[optimal_idx],
            'false_positive_rate': fpr[optimal_idx]
        }
```

### Explainable AI Integration

#### SHAP Integration

```python
class ModelExplainer:
    """Provides explanations for model predictions using SHAP and LIME."""
    
    def __init__(self, model: nn.Module, feature_names: List[str]):
        self.model = model
        self.feature_names = feature_names
        self.explainer = None
        self._setup_explainer()
    
    def _setup_explainer(self):
        """Setup SHAP explainer based on model type."""
        
        # Create a wrapper for PyTorch model
        def model_predict(x):
            with torch.no_grad():
                tensor_x = torch.FloatTensor(x)
                return self.model(tensor_x).numpy()
        
        # Initialize SHAP explainer
        self.explainer = shap.KernelExplainer(model_predict, self.background_data)
    
    def explain_prediction(
        self, 
        instance: pd.DataFrame,
        explanation_type: str = 'local'
    ) -> Dict[str, Any]:
        """Generate explanation for a prediction."""
        
        if explanation_type == 'local':
            return self._explain_local(instance)
        elif explanation_type == 'global':
            return self._explain_global()
        else:
            raise ValueError(f"Unknown explanation type: {explanation_type}")
    
    def _explain_local(self, instance: pd.DataFrame) -> Dict[str, Any]:
        """Generate local explanation for a single instance."""
        
        # Get SHAP values
        shap_values = self.explainer.shap_values(instance.values)
        
        # Get prediction
        prediction = self.model(torch.FloatTensor(instance.values)).item()
        
        # Create explanation
        explanation = {
            'prediction': prediction,
            'feature_contributions': dict(zip(self.feature_names, shap_values[0])),
            'top_features': self._get_top_features(shap_values[0]),
            'explanation_text': self._generate_explanation_text(shap_values[0], prediction)
        }
        
        return explanation
    
    def _generate_explanation_text(
        self, 
        shap_values: np.ndarray, 
        prediction: float
    ) -> str:
        """Generate human-readable explanation."""
        
        # Get top contributing features
        top_indices = np.argsort(np.abs(shap_values))[-5:]
        
        explanation_parts = []
        
        if prediction > 0.5:  # Anomaly detected
            explanation_parts.append("Esta transaÃ§Ã£o foi classificada como ANÃMALA devido aos seguintes fatores:")
        else:
            explanation_parts.append("Esta transaÃ§Ã£o foi classificada como NORMAL. Principais fatores:")
        
        for idx in reversed(top_indices):
            feature_name = self.feature_names[idx]
            contribution = shap_values[idx]
            
            if abs(contribution) > 0.01:  # Only significant contributions
                direction = "aumenta" if contribution > 0 else "diminui"
                explanation_parts.append(f"â¢ {feature_name} {direction} a probabilidade de anomalia")
        
        return "\n".join(explanation_parts)
```

### Model Serving

#### Production Deployment

```python
class ModelServer:
    """Production model serving with caching and monitoring."""
    
    def __init__(self, model_path: str, config: Dict[str, Any]):
        self.model = self._load_model(model_path)
        self.config = config
        self.cache = TTLCache(maxsize=1000, ttl=300)  # 5-minute cache
        self.metrics = ModelMetrics()
        
        # Monitoring
        self.prediction_counter = Counter('model_predictions_total', ['model_version', 'outcome'])
        self.latency_histogram = Histogram('model_inference_duration_seconds', ['model_version'])
    
    async def predict(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """Make prediction with caching and monitoring."""
        
        start_time = time.time()
        
        try:
            # Check cache
            cache_key = self._compute_cache_key(features)
            if cache_key in self.cache:
                cached_result = self.cache[cache_key]
                self.prediction_counter.labels(model_version=self.model.version, outcome='cached').inc()
                return cached_result
            
            # Preprocess features
            processed_features = self._preprocess_features(features)
            
            # Make prediction
            with torch.no_grad():
                tensor_features = torch.FloatTensor(processed_features)
                prediction = self.model(tensor_features).item()
            
            # Post-process result
            result = {
                'anomaly_score': prediction,
                'is_anomaly': prediction > self.config.get('threshold', 0.5),
                'confidence': self._calculate_confidence(prediction),
                'explanation': await self._generate_explanation(processed_features, prediction)
            }
            
            # Cache result
            self.cache[cache_key] = result
            
            # Metrics
            outcome = 'anomaly' if result['is_anomaly'] else 'normal'
            self.prediction_counter.labels(model_version=self.model.version, outcome=outcome).inc()
            
            return result
            
        finally:
            # Record latency
            duration = time.time() - start_time
            self.latency_histogram.labels(model_version=self.model.version).observe(duration)
```

## Performance Benchmarks

### Model Performance

| Model Type | Precision | Recall | F1-Score | AUC-ROC | Inference Time |
|------------|-----------|--------|----------|---------|----------------|
| Isolation Forest | 0.847 | 0.823 | 0.835 | 0.901 | 12ms |
| Autoencoder | 0.892 | 0.845 | 0.868 | 0.923 | 18ms |
| LSTM Detector | 0.876 | 0.834 | 0.854 | 0.915 | 24ms |
| **Ensemble** | **0.921** | **0.887** | **0.904** | **0.954** | **32ms** |

### Scalability Metrics

- **Throughput**: 3,000+ predictions/second
- **Memory Usage**: &lt;2GB per model instance
- **Model Size**: 45MB (compressed)
- **Training Time**: 2-4 hours on V100 GPU

## Best Practices

### Data Quality

```python
def validate_data_quality(df: pd.DataFrame) -> Dict[str, float]:
    """Comprehensive data quality assessment."""
    
    quality_metrics = {
        'completeness': 1 - df.isnull().sum().sum() / (df.shape[0] * df.shape[1]),
        'uniqueness': df.drop_duplicates().shape[0] / df.shape[0],
        'consistency': check_data_consistency(df),
        'validity': check_data_validity(df),
        'timeliness': check_data_timeliness(df)
    }
    
    overall_quality = np.mean(list(quality_metrics.values()))
    quality_metrics['overall'] = overall_quality
    
    return quality_metrics
```

### Model Monitoring

```python
class ModelDriftDetector:
    """Detects model and data drift in production."""
    
    def __init__(self, reference_data: pd.DataFrame):
        self.reference_data = reference_data
        self.drift_detectors = {
            'kolmogorov_smirnov': KSTest(),
            'population_stability_index': PSITest(),
            'jensen_shannon_distance': JSTest()
        }
    
    def detect_drift(self, current_data: pd.DataFrame) -> Dict[str, Any]:
        """Detect drift in incoming data."""
        
        drift_results = {}
        
        for feature in self.reference_data.columns:
            if feature in current_data.columns:
                feature_drift = {}
                
                for test_name, detector in self.drift_detectors.items():
                    drift_score = detector.calculate(
                        self.reference_data[feature],
                        current_data[feature]
                    )
                    feature_drift[test_name] = drift_score
                
                drift_results[feature] = feature_drift
        
        return drift_results
```

---

This ML pipeline documentation provides a comprehensive guide to the machine learning infrastructure powering the CidadÃ£o.AI anomaly detection system, ensuring reproducible, scalable, and maintainable model development and deployment.